The goal of the study is to present their new method for Hypothesis testing on the order of the normal mixture. They present an expectation-maximization (EM) test for testing the null hypothesis and they consider an arbitrary order m_0 under a finite normal mixture model. The simulation study assesses the accuracy of the proposed asymptotic approximation in finite samples and to examine the power of the EM test. They also look at the significance in the metric of p values in assessing their method. Their method applies a penalty function on the component variance to obtain a bounded penalized likelihood. It then assesses the improvement over the null models.

The choices they made for the simulation study was sample size (200-400), the number of repetitions (5000), signficance levels (5% and 1%). Moreover the EM test that they carry out is calculated based on the recommendations for the terms B, K, as well as the penalty functions p(β) and pn(σ 2; σˆ 2). And the power of the EM, under each alternative model is calculated, based on 1000 repetitions. The key aspects of the data generating mechanism - random numbers from the normal mixture - that will affect the statistical power of the test is the sample size (larger will increase the power). Also the significance level has an affect: the more stringent (lower) the significance level, the lower the power. Their penalty function contains a tuning parameter that affects the precision of the test. They solve the tuning problem via a novel computer experiment and provide an easy-to-use data-dependent formula. Another aspect of the data generating mechanism that will affect the power is how they choose parameters such as proportions, std and component mean.

Alternatives would be to increase the sample size significantly in order to affect the statistical power. We could also increase the number of replicates to further increase the simulation error. In the data generating scenarios they exclude the case where two normal mixture components have the same mean, because it is more challenging technically (it would be interesting to see). Also they could further change the component mean, std and other proportions, that would be useful and interesting to see. As well as larger models (with more parameters).

When setting up an experimental design analysis we could implement the standard strategy and discretize the inputs. Since the component mean varies this might be difficult however. If the number of inputs was small we could carry out a full factorial design. However the levels needs to be chosen in a reasonable way to not have too many treatment combinations. If we have a large a large number of inputs we could use the Latin hypercube approach (each input sampled uniformly).

Their figures and tables present boxplots indicating the significance level of the tests as well as the parameter selections and the powers of the EM for different models and sample sizes. They could present more metrics, such as the true p-values and some summary statistics. However what I really lack is they also should asses the magnitude of impact from different inputs by showing a decomposition of sums of squares, instead of only statistical significance. They do not clearly state that the simulation uncertainty / standard errors is satisfactory beyond doubt, even though they do state the simulation repetitions and the sample sizes. 


The tables 4 and 6 make sense in how the power varies, since the significance becomes greater for larger sample sizes, and also that it is more accurate for models with less parameters (when m is smaller). Moreover they state that "when the component means under the alternative models become far away from one another, the power of the test increases." Which also can be seen in the tables. However I would like to know how likely is it for EM to be of power 100 and why it varies quite drastically (maybe could have read the whole paper to figure this out). Moreover in generating the data for the simulation study they could have been more clear and presented the structure of real data and talk more about their distributional assumptions, dependence structure, outliers and random effects.

The JASA guidelines state that: 
“Results Based on Computation - Papers reporting results based on computation should provide enough information so that readers can evaluate the quality of the results. Such information includes estimated accuracy of results, as well as descriptions of pseudorandom-number generators, numerical algorithms, computers, programming languages, and major software components that were used.”

Overall they fulfill the criteria, however what I really lack is that they do not explicitly describe how they generate the pseudorandom numbers to obtain the simulated Type I errors. The data generating mechanism is only briefly described as sampling from the normal mixture distribution.